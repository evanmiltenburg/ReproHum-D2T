{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e500e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02930e90",
   "metadata": {},
   "source": [
    "# Krippendorff's alpha\n",
    "\n",
    "Here are four attempts to do essentially the same thing: generate data files in Python and analyze them using R. We follow [this tutorial](https://rpubs.com/jacoblong/content-analysis-krippendorff-alpha-R) to compute Krippendorff's alpha.\n",
    "\n",
    "## Attempt 1: create the CSV fully in Python\n",
    "\n",
    "We can first create a matrix where the rows are raters and columns are items. Cells contain ratings. This means we don't do very much in R; just computing Kalpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23cb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_for_dimension(dimension):\n",
    "    \"Create a CSV file for a specific dimension to compute Krippendorff's alpha.\"\n",
    "    answer_name = {\"Coherence\": \"Answer.best_coh\",\n",
    "                   \"Grammaticality\": \"Answer.best_grammar\",\n",
    "                   \"Repetition\": \"Answer.best_redun\"}\n",
    "    answer = answer_name[dimension]\n",
    "    index = defaultdict(dict)\n",
    "    for path in glob.glob(f\"./Responses/{dimension}/*.xlsx\"):\n",
    "        df = pd.read_excel(path)\n",
    "        for record in df.to_records():\n",
    "            # Add worker ID to the dictionary if it's not already in there.\n",
    "            if record['WorkerId'] not in index:\n",
    "                index[record['WorkerId']]['worker_id'] = record['WorkerId']\n",
    "            # Add response for a specific item to the dictionary.\n",
    "            index[record['WorkerId']][record['Input.code']] = record[answer].upper() # Ensure uppercase\n",
    "    # Convert all items in the index to a data frame.\n",
    "    # This data frame will be very sparse: each row represents the responses from one worker.\n",
    "    # That worker will only have rated a small subset of all items.\n",
    "    kalpha_table = pd.DataFrame(index.values())\n",
    "    kalpha_table.to_csv(f\"./Stats/kalpha1/{dimension}_kalpha.csv\" ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4e5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Krippendorff's alpha.\n",
    "for dimension in [\"Coherence\", \"Repetition\", \"Grammaticality\"]:\n",
    "    csv_for_dimension(dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1a645",
   "metadata": {},
   "source": [
    "We seem to be getting a very low alpha:\n",
    "\n",
    "* Coherence: 0.128\n",
    "* Grammaticality: 0.0363\n",
    "* Repetition: 0.179\n",
    "\n",
    "Maybe something is wrong? Maybe the issue is that the CSV is really sparse OR there was a coding issue. Otherwise the R script we have in folder `kalpha1` seems to do the job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5a07d",
   "metadata": {},
   "source": [
    "## Attempt 2: concatenate files and solve the rest in R\n",
    "\n",
    "We can also create a simple CSV with just the columns for Item ID, Worker ID, and Rating. This means we can follow the full tutorial in R, using the tidyverse commands to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398fac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_for_dimension(dimension):\n",
    "    \"Create a CSV file for a specific dimension to compute Krippendorff's alpha.\"\n",
    "    answer_name = {\"Coherence\": \"Answer.best_coh\",\n",
    "                   \"Grammaticality\": \"Answer.best_grammar\",\n",
    "                   \"Repetition\": \"Answer.best_redun\"}\n",
    "    answer = answer_name[dimension]\n",
    "    frames = []\n",
    "    for path in glob.glob(f\"./Responses/{dimension}/*.xlsx\"):\n",
    "        df = pd.read_excel(path)\n",
    "        frames.append(df)\n",
    "    df = pd.concat(frames)\n",
    "    kalpha_table = df.filter(['WorkerId', 'Input.code', answer], axis=1)\n",
    "    # Ensure uppercase:\n",
    "    kalpha_table[answer] = kalpha_table[answer].apply(lambda x:x.upper())\n",
    "    kalpha_table.to_csv(f\"./Stats/kalpha2/{dimension}_kalpha.csv\" ,index=False)\n",
    "\n",
    "# prepare Krippendorff's alpha.\n",
    "for dimension in [\"Coherence\", \"Repetition\", \"Grammaticality\"]:\n",
    "    csv_for_dimension(dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b594248",
   "metadata": {},
   "source": [
    "Alpha is exactly the same:\n",
    "\n",
    "* Coherence: 0.128 \n",
    "* Grammaticality: 0.0363 \n",
    "* Repetition: 0.179\n",
    "\n",
    "So clearly not a preprocessing issue, but by far not the alpha of 0.47 we see in the paper. What did the authors do to achieve this? Perhaps they created three virtual raters instead of using a very sparse matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b66783",
   "metadata": {},
   "source": [
    "## Attempt 3: remove bad responses\n",
    "\n",
    "One option is that there is a data issue. Maybe there are responses that are not equal to A or B? We can simply filter those out and see whether this makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079044a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_for_dimension(dimension):\n",
    "    \"Create a CSV file for a specific dimension to compute Krippendorff's alpha.\"\n",
    "    answer_name = {\"Coherence\": \"Answer.best_coh\",\n",
    "                   \"Grammaticality\": \"Answer.best_grammar\",\n",
    "                   \"Repetition\": \"Answer.best_redun\"}\n",
    "    answer = answer_name[dimension]\n",
    "    frames = []\n",
    "    for path in glob.glob(f\"./Responses/{dimension}/*.xlsx\"):\n",
    "        df = pd.read_excel(path)\n",
    "        frames.append(df)\n",
    "    df = pd.concat(frames)\n",
    "    kalpha_table = df.filter(['WorkerId', 'Input.code', answer], axis=1)\n",
    "    # Ensure uppercase:\n",
    "    kalpha_table[answer] = kalpha_table[answer].apply(lambda x:x.upper())\n",
    "    # Here comes the filtering step:\n",
    "    kalpha_table = kalpha_table[kalpha_table[answer].isin(['A','B'])]\n",
    "    kalpha_table.to_csv(f\"./Stats/kalpha3/{dimension}_kalpha.csv\" ,index=False)\n",
    "\n",
    "# prepare Krippendorff's alpha.\n",
    "for dimension in [\"Coherence\", \"Repetition\", \"Grammaticality\"]:\n",
    "    csv_for_dimension(dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926461b",
   "metadata": {},
   "source": [
    "Results from R:\n",
    "\n",
    "* Coherence: 0.131\n",
    "* Grammaticality: 0.0438 \n",
    "* Repetition: 0.203\n",
    "\n",
    "That does improve the scores a little, but it's otherwise not very impactful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e7386",
   "metadata": {},
   "source": [
    "#### Attempt 4: create a CSV file with only three coders, resulting in a more dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184cc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_for_dimension(dimension):\n",
    "    \"Create a CSV file for a specific dimension to compute Krippendorff's alpha.\"\n",
    "    answer_name = {\"Coherence\": \"Answer.best_coh\",\n",
    "                   \"Grammaticality\": \"Answer.best_grammar\",\n",
    "                   \"Repetition\": \"Answer.best_redun\"}\n",
    "    answer = answer_name[dimension]\n",
    "    frames = []\n",
    "    for path in glob.glob(f\"./Responses/{dimension}/*.xlsx\"):\n",
    "        df = pd.read_excel(path)\n",
    "        frames.append(df)\n",
    "    df = pd.concat(frames)\n",
    "    rating_index = defaultdict(list)\n",
    "    for record in df.to_records():\n",
    "        item = record['Input.code']\n",
    "        response = record[answer]\n",
    "        rating_index[item].append(response)\n",
    "    rows = []\n",
    "    for item, responses in rating_index.items():\n",
    "        for i, response in enumerate(responses):\n",
    "            rater = f\"rater{i}\"\n",
    "            rows.append(dict(rater=rater, response=response.upper(), item=item))\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(f\"./Stats/kalpha4/{dimension}_kalpha.csv\" ,index=False)\n",
    "\n",
    "for dimension in [\"Coherence\", \"Repetition\", \"Grammaticality\"]:\n",
    "    csv_for_dimension(dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fedd9",
   "metadata": {},
   "source": [
    "Running this through R (see scripts in directory) yields similar numbers as before:\n",
    "\n",
    "* Coherence: 0.128 \n",
    "* Grammaticality: 0.0355\n",
    "* Repetition: 0.178"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edadeda",
   "metadata": {},
   "source": [
    "# Percentage agreement\n",
    "\n",
    "Here's a way to compute percentage agreement with the majority of respondents, for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625ff156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_dimension(dimension):\n",
    "    \"Compute percentage agreement for a particular quality dimension.\"\n",
    "    answer_name = {\"Coherence\": \"Answer.best_coh\",\n",
    "                   \"Grammaticality\": \"Answer.best_grammar\",\n",
    "                   \"Repetition\": \"Answer.best_redun\"}\n",
    "    total_items = 600\n",
    "    answer = answer_name[dimension]\n",
    "    \n",
    "    # Build an index of items that workers looked at:\n",
    "    item_index = defaultdict(list)\n",
    "    # Build an index of answers that workers have given for each item:\n",
    "    answer_index = defaultdict(dict)\n",
    "\n",
    "    # For each batch...\n",
    "    for path in glob.glob(f\"./Responses/{dimension}/*.xlsx\"):\n",
    "        df = pd.read_excel(path)         # Read Excel\n",
    "        for record in df.to_records():   # Turn rows into records for easy addressing\n",
    "            # Set shorthand variables:\n",
    "            worker = record['WorkerId']\n",
    "            response = record[answer].upper() # Ensure uppercase.\n",
    "            item = record['Input.code']\n",
    "\n",
    "            # Add the item to the list.\n",
    "            item_index[worker].append(item)\n",
    "            \n",
    "            # Index the response.\n",
    "            answer_index[item][worker] = response\n",
    "\n",
    "    #####################################\n",
    "    # Now let's determine worker quality.\n",
    "    weighted_worker_quality = 0\n",
    "    \n",
    "    # Could have been a number as well, but we will return this dictionary\n",
    "    # for future reference. If we need to, we can easily check how good/bad\n",
    "    # workers' responses are.\n",
    "    worker_quality_index = dict()\n",
    "    for worker, items in item_index.items():\n",
    "        majority = []\n",
    "        for item in items:\n",
    "            worker_answer = answer_index[item][worker] # Get the relevant answer for this worker.\n",
    "            answers = list(answer_index[item].values())# Get all answers.\n",
    "            occurrences = answers.count(worker_answer) # Count how often the worker's answer occurs.\n",
    "\n",
    "            # There are three responses per item. 1 is the minority, 2 and 3 count as the majority.\n",
    "            # The answer restriction is in place to ensure spam does not reward the annotators.\n",
    "            if occurrences == 1 or worker_answer not in ['A','B']:\n",
    "                majority.append(0)\n",
    "            else:\n",
    "                majority.append(1)\n",
    "        \n",
    "        # The mean score is quite a crude indicator for workers who rated very few items.\n",
    "        # It gets more accurate with more items.\n",
    "        # But we just need an overall score, so this is not a big issue.\n",
    "        worker_quality = mean(majority)\n",
    "        worker_quality_index[worker] = worker_quality\n",
    "\n",
    "        # The weighted score is just updated with every worker.\n",
    "        weighted_worker_quality += (worker_quality * len(items))\n",
    "    \n",
    "    # Here we combine our two overall scores.\n",
    "    # 1. Average performance score. This should represent the quality of all workers.\n",
    "    # 2. Weighted average performance. \n",
    "    # The latter is fairer because workers who contribute more should also count more\n",
    "    # towards the overall quality.\n",
    "    avg_worker_quality = mean(worker_quality_index.values())\n",
    "    weighted_worker_quality = weighted_worker_quality/total_items\n",
    "    \n",
    "    return avg_worker_quality, weighted_worker_quality, worker_quality_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf09dcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Weighted Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coherence</td>\n",
       "      <td>0.717073</td>\n",
       "      <td>0.778333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repetition</td>\n",
       "      <td>0.734200</td>\n",
       "      <td>0.786667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grammaticality</td>\n",
       "      <td>0.737685</td>\n",
       "      <td>0.756667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Category      Mean  Weighted Mean\n",
       "0       Coherence  0.717073       0.778333\n",
       "1      Repetition  0.734200       0.786667\n",
       "2  Grammaticality  0.737685       0.756667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = [\"Category\", \"Mean\", \"Weighted Mean\"]\n",
    "data= []\n",
    "avg_worker_quality, weighted_worker_quality, worker_quality_index = analyse_dimension(\"Coherence\")\n",
    "data.append([\"Coherence\", avg_worker_quality, weighted_worker_quality])\n",
    "\n",
    "avg_worker_quality, weighted_worker_quality, worker_quality_index = analyse_dimension(\"Repetition\")\n",
    "data.append([\"Repetition\", avg_worker_quality, weighted_worker_quality])\n",
    "\n",
    "avg_worker_quality, weighted_worker_quality, worker_quality_index = analyse_dimension(\"Grammaticality\")\n",
    "data.append([\"Grammaticality\", avg_worker_quality, weighted_worker_quality])\n",
    "\n",
    "df = pd.DataFrame(columns=header, data=data)\n",
    "with open(\"./Tables/percentage_agreement.tex\", 'w') as f:\n",
    "    f.write(df.style.format(formatter=\"{:.2f}\".format, \n",
    "                            subset=[\"Mean\", \"Weighted Mean\"]).hide(axis=0).to_latex(hrules=True))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b1ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
